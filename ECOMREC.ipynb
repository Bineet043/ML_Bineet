{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/bineetbairagi/ecomrec?scriptVersionId=259155805\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":1,"id":"a7029ff7","metadata":{"execution":{"iopub.execute_input":"2025-08-31T04:47:43.006942Z","iopub.status.busy":"2025-08-31T04:47:43.006627Z","iopub.status.idle":"2025-08-31T04:49:44.482782Z","shell.execute_reply":"2025-08-31T04:49:44.481995Z"},"papermill":{"duration":121.481496,"end_time":"2025-08-31T04:49:44.48506","exception":false,"start_time":"2025-08-31T04:47:43.003564","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded dataset from KaggleHub: /kaggle/input/customer-shopping-latest-trends-dataset/shopping_trends.csv\n","Columns: ['customer id', 'age', 'gender', 'item purchased', 'category', 'purchase amount (usd)', 'location', 'size', 'color', 'season', 'review rating', 'subscription status', 'payment method', 'shipping type', 'discount applied', 'promo code used', 'previous purchases', 'preferred payment method', 'frequency of purchases']\n","Generated samples: (3897, 3)\n","Epoch 1: Loss 1.3027, Val Acc 0.4449\n","Epoch 2: Loss 1.2212, Val Acc 0.4449\n","Epoch 3: Loss 1.2201, Val Acc 0.4449\n","Epoch 4: Loss 1.2209, Val Acc 0.4449\n","Epoch 5: Loss 1.2204, Val Acc 0.4449\n","Example history: blue jeans running shoes laptop\n","Predicted next category: Clothing\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from datetime import datetime\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.feature_extraction.text import HashingVectorizer\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import os\n","import scipy.sparse as sp\n","\n","# Optional KaggleHub handling\n","try:\n","    import kagglehub\n","except Exception:\n","    kagglehub = None\n","\n","# --------------------------\n","# 0) Load Kaggle dataset robustly\n","# --------------------------\n","\n","DATASET_HANDLE = \"bhadramohit/customer-shopping-latest-trends-dataset\"\n","df = None\n","\n","try:\n","    if kagglehub is None:\n","        raise ImportError(\"kagglehub is not available.\")\n","\n","    path = kagglehub.dataset_download(DATASET_HANDLE)\n","    if isinstance(path, str) and os.path.isdir(path):\n","        dataset_dir = path\n","    elif isinstance(path, (list, tuple)) and len(path) > 0:\n","        dataset_dir = path[0]\n","    else:\n","        raise FileNotFoundError(\"Dataset directory not found.\")\n","\n","    # Load CSV\n","    csv_files = [f for f in os.listdir(dataset_dir) if f.endswith(\".csv\")]\n","    if not csv_files:\n","        raise FileNotFoundError(\"No CSV in dataset.\")\n","    file_path = os.path.join(dataset_dir, csv_files[0])\n","    df = pd.read_csv(file_path)\n","    print(f\"Loaded dataset from KaggleHub: {file_path}\")\n","\n","except Exception as e:\n","    print(f\"Failed to load dataset from KaggleHub. Error: {e}\")\n","    print(\"Using fallback toy data.\")\n","\n","    def generate_toy_data(n_users=50, n_events_per_user=20, seed=42):\n","        rng = np.random.default_rng(seed)\n","        categories = ['electronics', 'clothing', 'home', 'sports', 'books']\n","        queries_pool = [\n","            'blue jeans', 'running shoes', 'laptop', 'coffee maker', 'smartphone',\n","            'wireless headphones', 'winter coat', 'gaming mouse', 'kitchen blender', 'yoga mat'\n","        ]\n","        rows = []\n","        for user_id in range(n_users):\n","            for t in range(n_events_per_user):\n","                ts = datetime(2024, 1, 1).timestamp() + t*3600 + rng.integers(0, 3600)\n","                q1 = rng.choice(queries_pool)\n","                q2 = rng.choice(queries_pool)\n","                target = rng.choice(categories)\n","                rows.append({\n","                    'user_id': f'u{user_id}',\n","                    'timestamp': ts,\n","                    'queries': [q1, q2],\n","                    'target_category': target\n","                })\n","        return pd.DataFrame(rows)\n","\n","    df = generate_toy_data()\n","\n","# --------------------------\n","# 1) Normalize text + targets\n","# --------------------------\n","\n","df.columns = df.columns.astype(str).str.strip().str.lower()\n","print(\"Columns:\", df.columns.tolist())\n","\n","def to_text(val):\n","    if isinstance(val, str):\n","        return val\n","    try:\n","        return ' '.join(val)\n","    except Exception:\n","        return str(val)\n","\n","if 'queries' in df.columns:\n","    df['text'] = df['queries'].apply(to_text)\n","    target_col = 'target_category'\n","elif 'category' in df.columns:\n","    df['text'] = df['category'].astype(str)\n","    target_col = 'category'\n","else:\n","    raise KeyError(\"No suitable text column found.\")\n","\n","# --------------------------\n","# 1.5) Add timestamp for ordering\n","# --------------------------\n","\n","if 'timestamp' not in df.columns:\n","    if 'invoice_date' in df.columns:\n","        df['timestamp'] = pd.to_datetime(df['invoice_date'], errors='coerce').astype(int) / 1e9\n","    else:\n","        df['timestamp'] = np.arange(len(df))\n","\n","if 'user_id' not in df.columns:\n","    if 'customer_id' in df.columns:\n","        df['user_id'] = df['customer_id'].astype(str)\n","    else:\n","        df['user_id'] = \"global_user\"\n","\n","# --------------------------\n","# 2) Build history-based samples\n","# --------------------------\n","\n","history_len = 3\n","df = df.sort_values(['user_id', 'timestamp']).reset_index(drop=True)\n","\n","def build_history_features(user_id, group):\n","    texts = group['text'].tolist()\n","    targets = group[target_col].astype(str).tolist()\n","    samples = []\n","    for i in range(history_len, len(group)):\n","        history_concat = ' '.join(texts[i-history_len:i])\n","        target = targets[i]\n","        samples.append({\n","            'user_id': user_id,\n","            'history_text': history_concat,\n","            'target_category': target\n","        })\n","    return pd.DataFrame(samples)\n","\n","samples_df = pd.concat([\n","    build_history_features(uid, g) for uid, g in df.groupby('user_id') if len(g) > history_len\n","], ignore_index=True)\n","\n","print(\"Generated samples:\", samples_df.shape)\n","\n","X_text = samples_df['history_text'].values\n","y_targets = samples_df['target_category'].values\n","\n","le = LabelEncoder()\n","y = le.fit_transform(y_targets)\n","class_names = le.classes_\n","\n","vectorizer = HashingVectorizer(n_features=2**20, alternate_sign=False, lowercase=True)\n","X_hashed = vectorizer.transform(X_text)\n","\n","X_train, X_val, y_train, y_val = train_test_split(X_hashed, y, test_size=0.2, random_state=42, stratify=y)\n","\n","# --------------------------\n","# 3) PyTorch Dataset + Model\n","# --------------------------\n","\n","def sparse_collate_fn(batch):\n","    data = sp.vstack([item[0] for item in batch])\n","    targets = torch.tensor([item[1] for item in batch], dtype=torch.long)\n","    return torch.tensor(data.toarray(), dtype=torch.float32), targets\n","\n","class TextDataset(Dataset):\n","    def __init__(self, X, y):\n","        self.X, self.y = X, y\n","    def __len__(self):\n","        return self.X.shape[0]\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]\n","\n","train_loader = DataLoader(TextDataset(X_train, y_train), batch_size=64, shuffle=True, collate_fn=sparse_collate_fn)\n","val_loader = DataLoader(TextDataset(X_val, y_val), batch_size=64, shuffle=False, collate_fn=sparse_collate_fn)\n","\n","class SimpleNet(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, n_classes):\n","        super().__init__()\n","        self.fc1 = nn.Linear(input_dim, hidden_dim)\n","        self.relu = nn.ReLU()\n","        self.drop = nn.Dropout(0.5)\n","        self.fc2 = nn.Linear(hidden_dim, n_classes)\n","    def forward(self, x):\n","        return self.fc2(self.drop(self.relu(self.fc1(x))))\n","\n","model = SimpleNet(2**20, 256, len(class_names))\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","\n","# --------------------------\n","# 4) Train & Evaluate\n","# --------------------------\n","\n","def train_one_epoch():\n","    model.train()\n","    total_loss = 0\n","    for Xb, yb in train_loader:\n","        Xb, yb = Xb.to(device), yb.to(device)\n","        optimizer.zero_grad()\n","        loss = criterion(model(Xb), yb)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item() * Xb.size(0)\n","    return total_loss / len(train_loader.dataset)\n","\n","def evaluate(loader):\n","    model.eval()\n","    correct, total = 0, 0\n","    with torch.no_grad():\n","        for Xb, yb in loader:\n","            Xb, yb = Xb.to(device), yb.to(device)\n","            preds = model(Xb).argmax(1)\n","            correct += (preds == yb).sum().item()\n","            total += yb.size(0)\n","    return correct / total\n","\n","for epoch in range(1, 6):\n","    loss = train_one_epoch()\n","    acc = evaluate(val_loader)\n","    print(f\"Epoch {epoch}: Loss {loss:.4f}, Val Acc {acc:.4f}\")\n","\n","# --------------------------\n","# 5) Inference Helper\n","# --------------------------\n","\n","def predict_next_category(history_text):\n","    vec = vectorizer.transform([history_text]).toarray().astype(np.float32)\n","    vec_tensor = torch.tensor(vec, dtype=torch.float32).to(device)\n","    model.eval()\n","    with torch.no_grad():\n","        pred_idx = model(vec_tensor).argmax(1).item()\n","    return class_names[pred_idx]\n","\n","example = \"blue jeans running shoes laptop\"\n","print(\"Example history:\", example)\n","print(\"Predicted next category:\", predict_next_category(example))\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":6149964,"sourceId":9992547,"sourceType":"datasetVersion"}],"dockerImageVersionId":31090,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"papermill":{"default_parameters":{},"duration":127.141677,"end_time":"2025-08-31T04:49:46.007824","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-08-31T04:47:38.866147","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}