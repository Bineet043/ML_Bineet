{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/bineetbairagi/shop-pred?scriptVersionId=259154945\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":1,"id":"0a34d087","metadata":{"execution":{"iopub.execute_input":"2025-08-31T04:38:47.770033Z","iopub.status.busy":"2025-08-31T04:38:47.769736Z","iopub.status.idle":"2025-08-31T04:40:48.43688Z","shell.execute_reply":"2025-08-31T04:40:48.436163Z"},"papermill":{"duration":120.673395,"end_time":"2025-08-31T04:40:48.439778","exception":false,"start_time":"2025-08-31T04:38:47.766383","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading dataset CSV: /kaggle/input/customer-shopping-latest-trends-dataset/shopping_trends.csv\n","Loaded dataset from KaggleHub: /kaggle/input/customer-shopping-latest-trends-dataset/shopping_trends.csv\n","Columns after normalization: ['customer id', 'age', 'gender', 'item purchased', 'category', 'purchase amount (usd)', 'location', 'size', 'color', 'season', 'review rating', 'subscription status', 'payment method', 'shipping type', 'discount applied', 'promo code used', 'previous purchases', 'preferred payment method', 'frequency of purchases']\n","Text field created. Sample:\n","0    Clothing\n","1    Clothing\n","2    Clothing\n","3    Footwear\n","4    Clothing\n","Name: text, dtype: object\n","Target column present: True\n","Classes: ['Accessories', 'Clothing', 'Footwear', 'Outerwear'] Number of classes: 4\n","Epoch 1/5 - Loss: 1.1848 - Val Acc: 1.0000\n","Epoch 2/5 - Loss: 0.4931 - Val Acc: 1.0000\n","Epoch 3/5 - Loss: 0.1337 - Val Acc: 1.0000\n","Epoch 4/5 - Loss: 0.0488 - Val Acc: 1.0000\n","Epoch 5/5 - Loss: 0.0256 - Val Acc: 1.0000\n","Sample text: Clothing\n","Predicted category: Clothing\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from datetime import datetime\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.feature_extraction.text import HashingVectorizer\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import os\n","import scipy.sparse as sp\n","import glob\n","\n","# Optional: KaggleHub helper (import may vary by environment)\n","try:\n","    import kagglehub\n","except Exception:\n","    kagglehub = None\n","\n","# --------------------------\n","# 0) Load Kaggle dataset robustly\n","# --------------------------\n","\n","# User-configurable\n","DATASET_HANDLE = \"bhadramohit/customer-shopping-latest-trends-dataset\"\n","# EXPECTED_FILE is a hint; we will actually discover CSVs in the dataset\n","EXPECTED_FILE = \"customer_shopping_data.csv\"\n","\n","def find_csv_in_dataset(dataset_path):\n","    \"\"\"\n","    Recursively find candidate CSV files under dataset_path.\n","    Returns a list of full file paths.\n","    \"\"\"\n","    csv_files = []\n","    for root, dirs, files in os.walk(dataset_path):\n","        for f in files:\n","            if f.lower().endswith('.csv'):\n","                csv_files.append(os.path.join(root, f))\n","    return csv_files\n","\n","def load_dataframe_from_csv(file_path):\n","    print(f\"Loading dataset CSV: {file_path}\")\n","    return pd.read_csv(file_path)\n","\n","df = None\n","\n","try:\n","    if kagglehub is None:\n","        raise ImportError(\"kagglehub is not available in this environment.\")\n","\n","    # Attempt to download the dataset\n","    # Note: Some environments require different call signatures.\n","    # Here, we try a flexible approach that works with the provided API.\n","    path = None\n","    try:\n","        # Some versions support dataset_download(handle, filename=None)\n","        path = kagglehub.dataset_download(DATASET_HANDLE)  # no specific file\n","    except TypeError:\n","        # Fallback signature\n","        path = kagglehub.dataset_download(DATASET_HANDLE, None)\n","\n","    if isinstance(path, str) and os.path.isdir(path):\n","        dataset_dir = path\n","    else:\n","        # If the API returns a different structure, try to coerce to a directory\n","        dataset_dir = None\n","        if isinstance(path, (list, tuple)) and len(path) > 0:\n","            candidate = path[0]\n","            if isinstance(candidate, str) and os.path.isdir(candidate):\n","                dataset_dir = candidate\n","\n","    if dataset_dir is None:\n","        raise FileNotFoundError(\"Dataset directory not found after download.\")\n","\n","    # Discover CSV files inside the dataset directory\n","    csv_candidates = find_csv_in_dataset(dataset_dir)\n","\n","    if len(csv_candidates) == 0:\n","        raise FileNotFoundError(\"No CSV files found in the downloaded dataset directory.\")\n","\n","    if len(csv_candidates) == 1:\n","        file_path = csv_candidates[0]\n","    else:\n","        # Heuristic: prefer files with 'shopping' or 'data' in the filename\n","        candidates_sorted = sorted(\n","            csv_candidates,\n","            key=lambda p: (\n","                ('shopping' in os.path.basename(p).lower()) * 1 +\n","                ('data' in os.path.basename(p).lower()) * 1\n","            ),\n","            reverse=True\n","        )\n","        file_path = candidates_sorted[0]\n","\n","    df = load_dataframe_from_csv(file_path)\n","    print(f\"Loaded dataset from KaggleHub: {file_path}\")\n","\n","except Exception as e:\n","    print(f\"Failed to load dataset from KaggleHub. Error: {e}\")\n","    print(\"Using fallback toy data.\")\n","\n","    # Fallback toy data to keep the workflow executable\n","    def generate_toy_data(n_users=50, n_events_per_user=20, seed=42):\n","        rng = np.random.default_rng(seed)\n","        categories = ['electronics', 'clothing', 'home', 'sports', 'books']\n","        queries_pool = [\n","            'blue jeans', 'running shoes', 'laptop', 'coffee maker', 'smartphone',\n","            'wireless headphones', 'winter coat', 'gaming mouse', 'kitchen blender', 'yoga mat'\n","        ]\n","        rows = []\n","        for user_id in range(n_users):\n","            for t in range(n_events_per_user):\n","                ts = datetime(2024, 1, 1).timestamp() + t*3600 + rng.integers(0, 3600)\n","                q1 = rng.choice(queries_pool)\n","                q2 = rng.choice(queries_pool)\n","                queries = [q1, q2]\n","                target = rng.choice(categories)\n","                rows.append({\n","                    'user_id': f'u{user_id}',\n","                    'timestamp': ts,\n","                    'queries': queries,\n","                    'target_category': target\n","                })\n","        return pd.DataFrame(rows)\n","\n","    df = generate_toy_data()\n","\n","# --------------------------\n","# 1) Normalize to a single text field\n","# --------------------------\n","\n","# Normalize column names: strip spaces and lowercase for robust handling\n","df.columns = df.columns.astype(str)\n","df.columns = df.columns.str.strip().str.lower()\n","\n","print(\"Columns after normalization:\", df.columns.tolist())\n","\n","# Helper to convert a list of queries to a single text field\n","def to_text(val):\n","    if isinstance(val, str):\n","        return val\n","    try:\n","        return ' '.join(val)\n","    except TypeError:\n","        try:\n","            return ' '.join(str(x) for x in val)\n","        except Exception:\n","            return str(val)\n","\n","# Use 'category' for the real data, and 'queries' for the fallback toy data\n","if 'category' in df.columns:\n","    df['text'] = df['category'].astype(str)\n","    target_col = 'category'\n","elif 'queries' in df.columns:\n","    df['text'] = df['queries'].apply(to_text)\n","    target_col = 'target_category'\n","else:\n","    raise KeyError(\"No suitable text column found in the dataframe. Expected 'queries' or 'category'.\")\n","\n","print(\"Text field created. Sample:\")\n","print(df['text'].head())\n","print(\"Target column present:\", target_col in df.columns)\n","\n","# --------------------------\n","# 2) Prepare data for modeling\n","# --------------------------\n","\n","# Encode targets\n","le = LabelEncoder()\n","y = le.fit_transform(df[target_col].astype(str))\n","class_names = list(le.classes_)\n","n_classes = len(class_names)\n","print(\"Classes:\", class_names, \"Number of classes:\", n_classes)\n","\n","# Features: text -> hashing vectorization, keeping it sparse\n","vectorizer = HashingVectorizer(n_features=2**20, alternate_sign=False, lowercase=True)\n","X_hashed = vectorizer.transform(df['text'].values)\n","\n","# Train/test split\n","X_train, X_val, y_train, y_val = train_test_split(X_hashed, y, test_size=0.2, random_state=42, stratify=y)\n","\n","# --------------------------\n","# 3) PyTorch dataset/dataloader\n","# --------------------------\n","\n","def sparse_collate_fn(batch):\n","    data = sp.vstack([item[0] for item in batch])\n","    targets = torch.tensor([item[1] for item in batch], dtype=torch.long)\n","    return torch.tensor(data.toarray(), dtype=torch.float32), targets\n","\n","class TextDataset(Dataset):\n","    def __init__(self, X, y):\n","        self.X = X\n","        self.y = y\n","\n","    def __len__(self):\n","        return self.X.shape[0]\n","\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]\n","\n","train_ds = TextDataset(X_train, y_train)\n","val_ds = TextDataset(X_val, y_val)\n","\n","train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, collate_fn=sparse_collate_fn)\n","val_loader = DataLoader(val_ds, batch_size=64, shuffle=False, collate_fn=sparse_collate_fn)\n","\n","# --------------------------\n","# 4) Simple model: feed-forward over hashed features\n","# --------------------------\n","\n","class SimpleNet(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, n_classes):\n","        super(SimpleNet, self).__init__()\n","        self.fc1 = nn.Linear(input_dim, hidden_dim)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(0.5)\n","        self.fc2 = nn.Linear(hidden_dim, n_classes)\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","        return x\n","\n","# Note: input_dim must match the hashed feature dimension (2**20)\n","input_dim = 2**20\n","hidden_dim = 256\n","model = SimpleNet(input_dim, hidden_dim, n_classes)\n","\n","# Training settings\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","\n","# --------------------------\n","# 5) Training loop\n","# --------------------------\n","\n","def train_one_epoch():\n","    model.train()\n","    total_loss = 0.0\n","    for batch_X, batch_y in train_loader:\n","        batch_X = batch_X.to(device)\n","        batch_y = batch_y.to(device)\n","\n","        optimizer.zero_grad()\n","        logits = model(batch_X)\n","        loss = criterion(logits, batch_y)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item() * batch_X.size(0)\n","    return total_loss / len(train_loader.dataset)\n","\n","def evaluate(loader):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for batch_X, batch_y in loader:\n","            batch_X = batch_X.to(device)\n","            batch_y = batch_y.to(device)\n","            logits = model(batch_X)\n","            preds = torch.argmax(logits, dim=1)\n","            correct += (preds == batch_y).sum().item()\n","            total += batch_y.size(0)\n","    return correct / total\n","\n","n_epochs = 5\n","for epoch in range(1, n_epochs + 1):\n","    train_loss = train_one_epoch()\n","    val_acc = evaluate(val_loader)\n","    print(f\"Epoch {epoch}/{n_epochs} - Loss: {train_loss:.4f} - Val Acc: {val_acc:.4f}\")\n","\n","# --------------------------\n","# 6) Inference helper\n","# --------------------------\n","\n","def predict_text(text_sample):\n","    vec = vectorizer.transform([text_sample]).toarray().astype(np.float32)\n","    vec_tensor = torch.tensor(vec, dtype=torch.float32).to(device)\n","    model.eval()\n","    with torch.no_grad():\n","        logits = model(vec_tensor)\n","        pred_idx = int(torch.argmax(logits, dim=1).item())\n","    return class_names[pred_idx]\n","\n","# Example usage after training\n","sample_text = df['text'].iloc[0]\n","print(\"Sample text:\", sample_text)\n","print(\"Predicted category:\", predict_text(sample_text))"]},{"cell_type":"code","execution_count":2,"id":"1560584c","metadata":{"execution":{"iopub.execute_input":"2025-08-31T04:40:48.444942Z","iopub.status.busy":"2025-08-31T04:40:48.444501Z","iopub.status.idle":"2025-08-31T04:42:36.888427Z","shell.execute_reply":"2025-08-31T04:42:36.887589Z"},"papermill":{"duration":108.448019,"end_time":"2025-08-31T04:42:36.889752","exception":false,"start_time":"2025-08-31T04:40:48.441733","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded dataset from KaggleHub: /kaggle/input/customer-shopping-latest-trends-dataset/shopping_trends.csv\n","Columns: ['customer id', 'age', 'gender', 'item purchased', 'category', 'purchase amount (usd)', 'location', 'size', 'color', 'season', 'review rating', 'subscription status', 'payment method', 'shipping type', 'discount applied', 'promo code used', 'previous purchases', 'preferred payment method', 'frequency of purchases']\n","Generated samples: (3897, 3)\n","Epoch 1: Loss 1.3044, Val Acc 0.4449\n","Epoch 2: Loss 1.2249, Val Acc 0.4449\n","Epoch 3: Loss 1.2228, Val Acc 0.4449\n","Epoch 4: Loss 1.2222, Val Acc 0.4449\n","Epoch 5: Loss 1.2177, Val Acc 0.4449\n","Example history: blue jeans running shoes laptop\n","Predicted next category: Clothing\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from datetime import datetime\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.feature_extraction.text import HashingVectorizer\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import os\n","import scipy.sparse as sp\n","\n","# Optional KaggleHub handling\n","try:\n","    import kagglehub\n","except Exception:\n","    kagglehub = None\n","\n","# --------------------------\n","# 0) Load Kaggle dataset robustly\n","# --------------------------\n","\n","DATASET_HANDLE = \"bhadramohit/customer-shopping-latest-trends-dataset\"\n","df = None\n","\n","try:\n","    if kagglehub is None:\n","        raise ImportError(\"kagglehub is not available.\")\n","\n","    path = kagglehub.dataset_download(DATASET_HANDLE)\n","    if isinstance(path, str) and os.path.isdir(path):\n","        dataset_dir = path\n","    elif isinstance(path, (list, tuple)) and len(path) > 0:\n","        dataset_dir = path[0]\n","    else:\n","        raise FileNotFoundError(\"Dataset directory not found.\")\n","\n","    # Load CSV\n","    csv_files = [f for f in os.listdir(dataset_dir) if f.endswith(\".csv\")]\n","    if not csv_files:\n","        raise FileNotFoundError(\"No CSV in dataset.\")\n","    file_path = os.path.join(dataset_dir, csv_files[0])\n","    df = pd.read_csv(file_path)\n","    print(f\"Loaded dataset from KaggleHub: {file_path}\")\n","\n","except Exception as e:\n","    print(f\"Failed to load dataset from KaggleHub. Error: {e}\")\n","    print(\"Using fallback toy data.\")\n","\n","    def generate_toy_data(n_users=50, n_events_per_user=20, seed=42):\n","        rng = np.random.default_rng(seed)\n","        categories = ['electronics', 'clothing', 'home', 'sports', 'books']\n","        queries_pool = [\n","            'blue jeans', 'running shoes', 'laptop', 'coffee maker', 'smartphone',\n","            'wireless headphones', 'winter coat', 'gaming mouse', 'kitchen blender', 'yoga mat'\n","        ]\n","        rows = []\n","        for user_id in range(n_users):\n","            for t in range(n_events_per_user):\n","                ts = datetime(2024, 1, 1).timestamp() + t*3600 + rng.integers(0, 3600)\n","                q1 = rng.choice(queries_pool)\n","                q2 = rng.choice(queries_pool)\n","                target = rng.choice(categories)\n","                rows.append({\n","                    'user_id': f'u{user_id}',\n","                    'timestamp': ts,\n","                    'queries': [q1, q2],\n","                    'target_category': target\n","                })\n","        return pd.DataFrame(rows)\n","\n","    df = generate_toy_data()\n","\n","# --------------------------\n","# 1) Normalize text + targets\n","# --------------------------\n","\n","df.columns = df.columns.astype(str).str.strip().str.lower()\n","print(\"Columns:\", df.columns.tolist())\n","\n","def to_text(val):\n","    if isinstance(val, str):\n","        return val\n","    try:\n","        return ' '.join(val)\n","    except Exception:\n","        return str(val)\n","\n","if 'queries' in df.columns:\n","    df['text'] = df['queries'].apply(to_text)\n","    target_col = 'target_category'\n","elif 'category' in df.columns:\n","    df['text'] = df['category'].astype(str)\n","    target_col = 'category'\n","else:\n","    raise KeyError(\"No suitable text column found.\")\n","\n","# --------------------------\n","# 1.5) Add timestamp for ordering\n","# --------------------------\n","\n","if 'timestamp' not in df.columns:\n","    if 'invoice_date' in df.columns:\n","        df['timestamp'] = pd.to_datetime(df['invoice_date'], errors='coerce').astype(int) / 1e9\n","    else:\n","        df['timestamp'] = np.arange(len(df))\n","\n","if 'user_id' not in df.columns:\n","    if 'customer_id' in df.columns:\n","        df['user_id'] = df['customer_id'].astype(str)\n","    else:\n","        df['user_id'] = \"global_user\"\n","\n","# --------------------------\n","# 2) Build history-based samples\n","# --------------------------\n","\n","history_len = 3\n","df = df.sort_values(['user_id', 'timestamp']).reset_index(drop=True)\n","\n","def build_history_features(user_id, group):\n","    texts = group['text'].tolist()\n","    targets = group[target_col].astype(str).tolist()\n","    samples = []\n","    for i in range(history_len, len(group)):\n","        history_concat = ' '.join(texts[i-history_len:i])\n","        target = targets[i]\n","        samples.append({\n","            'user_id': user_id,\n","            'history_text': history_concat,\n","            'target_category': target\n","        })\n","    return pd.DataFrame(samples)\n","\n","samples_df = pd.concat([\n","    build_history_features(uid, g) for uid, g in df.groupby('user_id') if len(g) > history_len\n","], ignore_index=True)\n","\n","print(\"Generated samples:\", samples_df.shape)\n","\n","X_text = samples_df['history_text'].values\n","y_targets = samples_df['target_category'].values\n","\n","le = LabelEncoder()\n","y = le.fit_transform(y_targets)\n","class_names = le.classes_\n","\n","vectorizer = HashingVectorizer(n_features=2**20, alternate_sign=False, lowercase=True)\n","X_hashed = vectorizer.transform(X_text)\n","\n","X_train, X_val, y_train, y_val = train_test_split(X_hashed, y, test_size=0.2, random_state=42, stratify=y)\n","\n","# --------------------------\n","# 3) PyTorch Dataset + Model\n","# --------------------------\n","\n","def sparse_collate_fn(batch):\n","    data = sp.vstack([item[0] for item in batch])\n","    targets = torch.tensor([item[1] for item in batch], dtype=torch.long)\n","    return torch.tensor(data.toarray(), dtype=torch.float32), targets\n","\n","class TextDataset(Dataset):\n","    def __init__(self, X, y):\n","        self.X, self.y = X, y\n","    def __len__(self):\n","        return self.X.shape[0]\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]\n","\n","train_loader = DataLoader(TextDataset(X_train, y_train), batch_size=64, shuffle=True, collate_fn=sparse_collate_fn)\n","val_loader = DataLoader(TextDataset(X_val, y_val), batch_size=64, shuffle=False, collate_fn=sparse_collate_fn)\n","\n","class SimpleNet(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, n_classes):\n","        super().__init__()\n","        self.fc1 = nn.Linear(input_dim, hidden_dim)\n","        self.relu = nn.ReLU()\n","        self.drop = nn.Dropout(0.5)\n","        self.fc2 = nn.Linear(hidden_dim, n_classes)\n","    def forward(self, x):\n","        return self.fc2(self.drop(self.relu(self.fc1(x))))\n","\n","model = SimpleNet(2**20, 256, len(class_names))\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","\n","# --------------------------\n","# 4) Train & Evaluate\n","# --------------------------\n","\n","def train_one_epoch():\n","    model.train()\n","    total_loss = 0\n","    for Xb, yb in train_loader:\n","        Xb, yb = Xb.to(device), yb.to(device)\n","        optimizer.zero_grad()\n","        loss = criterion(model(Xb), yb)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item() * Xb.size(0)\n","    return total_loss / len(train_loader.dataset)\n","\n","def evaluate(loader):\n","    model.eval()\n","    correct, total = 0, 0\n","    with torch.no_grad():\n","        for Xb, yb in loader:\n","            Xb, yb = Xb.to(device), yb.to(device)\n","            preds = model(Xb).argmax(1)\n","            correct += (preds == yb).sum().item()\n","            total += yb.size(0)\n","    return correct / total\n","\n","for epoch in range(1, 6):\n","    loss = train_one_epoch()\n","    acc = evaluate(val_loader)\n","    print(f\"Epoch {epoch}: Loss {loss:.4f}, Val Acc {acc:.4f}\")\n","\n","# --------------------------\n","# 5) Inference Helper\n","# --------------------------\n","\n","def predict_next_category(history_text):\n","    vec = vectorizer.transform([history_text]).toarray().astype(np.float32)\n","    vec_tensor = torch.tensor(vec, dtype=torch.float32).to(device)\n","    model.eval()\n","    with torch.no_grad():\n","        pred_idx = model(vec_tensor).argmax(1).item()\n","    return class_names[pred_idx]\n","\n","example = \"blue jeans running shoes laptop\"\n","print(\"Example history:\", example)\n","print(\"Predicted next category:\", predict_next_category(example))\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":6149964,"sourceId":9992547,"sourceType":"datasetVersion"}],"dockerImageVersionId":31090,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"papermill":{"default_parameters":{},"duration":235.012918,"end_time":"2025-08-31T04:42:38.612978","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-08-31T04:38:43.60006","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}